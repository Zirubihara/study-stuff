# Python Data Processing Speed Test ğŸš€

**TL;DR**: This project tests how fast different Python libraries process large datasets. **Spoiler alert: Polars wins by a lot!**

## What This Project Does

Imagine you have a huge CSV file with millions of rows (like sales data, user logs, etc.). This project tests 5 different Python libraries to see which one processes the data fastest:

- **Pandas** ğŸ¼ - The most popular one everyone knows
- **Polars** âš¡ - The new super-fast kid on the block
- **PyArrow** ğŸ¹ - Good for pure numbers
- **Dask** ğŸŒªï¸ - For when your data is too big for memory
- **PySpark** âš¡ - For truly massive datasets

## ğŸ† The Results (Spoiler: Polars Dominates!)

We tested all libraries on different dataset sizes. Here's what we found:

### ğŸ“Š Speed Comparison

| Dataset Size | Winner | 2nd Place | Pandas Time | Winner Time | Speed Boost |
|-------------|--------|-----------|-------------|-------------|-------------|
| 1M rows     | **Polars** | PyArrow   | 1.51s       | 0.42s       | **3.6x faster** |
| 5M rows     | **Polars** | PyArrow   | 7.69s       | 1.84s       | **4.2x faster** |
| 10M rows    | **Polars** | PyArrow   | 16.95s      | 2.87s       | **5.9x faster** |
| 50M rows    | **Polars** | PyArrow   | 122.75s     | 22.95s      | **5.3x faster** |

### ğŸ’¾ Memory Usage

**Polars uses 50-60% less memory than Pandas!**

- 10M rows: Pandas = 1.46GB, Polars = 0.55GB
- 50M rows: Pandas = 7.22GB, Polars = 2.66GB

## ğŸ¯ When Should You Use What?

### Use **Polars** when:
- âœ… You want the fastest performance (almost always)
- âœ… You care about memory usage
- âœ… You have any dataset from 100K to 50M+ rows
- âœ… You want modern, clean syntax

### Use **Pandas** when:
- âœ… You're working with small datasets (< 1M rows)
- âœ… Your team already knows Pandas well
- âœ… You need specific Pandas-only features
- âš ï¸ **Warning**: Gets slow and memory-hungry with large data

### Use **PyArrow** when:
- âœ… You have pure numerical data (no text processing)
- âœ… You need to work with Apache ecosystem tools
- âœ… You want good performance with numerical operations

### Use **PySpark** when:
- âœ… You have truly massive datasets (100M+ rows)
- âœ… You have a cluster of computers
- âœ… You need distributed processing

### Use **Dask** when:
- âœ… Your data doesn't fit in memory
- âœ… You want to scale existing Pandas code
- âš ï¸ **Note**: Usually slower than other options

## ğŸš€ Quick Start - Run the Tests Yourself!

### Step 1: Setup (5 minutes)

```bash
# 1. Clone this project
git clone <your-repo-url>
cd study-stuff

# 2. Create a virtual environment (recommended)
python -m venv venv

# 3. Activate it
# On Windows:
venv\Scripts\activate
# On Mac/Linux:
source venv/bin/activate

# 4. Install required packages
pip install -r requirements.txt
```

### Step 2: Run Benchmarks (10-20 minutes total)

```bash
# Navigate to the benchmark folder
cd scripts/benchmarks/dataset_specific

# Run tests on different dataset sizes
python benchmark_1m_simple.py     # Takes ~30 seconds
python benchmark_5m_simple.py     # Takes ~2 minutes
python benchmark_10m_simple.py    # Takes ~3 minutes
python benchmark_50m_simple.py    # Takes ~8 minutes
python benchmark_100m_simple.py   # Takes ~15 minutes (for big data comparison)
```

### Step 3: See Your Results

```bash
# Go back to main folder
cd ../../..

# Look at the results
ls results/

# You'll see files like:
# performance_metrics_polars_10m.json
# performance_metrics_pandas_10m.json
# etc.
```

## ğŸ“ Where to Find Everything

```
study-stuff/
â”œâ”€â”€ scripts/benchmarks/dataset_specific/    â† The benchmark tests are here
â”œâ”€â”€ results/                               â† Results appear here as JSON files
â”œâ”€â”€ data/                                  â† CSV datasets are stored here
â”œâ”€â”€ charts/                                â† Generated charts go here
â””â”€â”€ README.md                              â† You are here!
```

## ğŸ“Š Understanding the Results

Each test creates a JSON file with timing results. Here's what the numbers mean:

```json
{
    "total_operation_time_seconds": 2.87,    â† Total time to process everything
    "loading_time_seconds": 0.503,          â† Time to load the CSV file
    "cleaning_time_seconds": 0.005,         â† Time to clean missing data
    "aggregation_time_seconds": 0.780,      â† Time to group and calculate averages
    "sorting_time_seconds": 1.008,          â† Time to sort the data
    "filtering_time_seconds": 0.057,        â† Time to filter rows
    "correlation_time_seconds": 0.515,      â† Time to calculate correlations
    "memory_size_gb": 0.549,                â† Memory used (in GB)
    "row_count": 10000000                   â† Number of rows processed
}
```

**Lower numbers = faster performance! ğŸƒâ€â™‚ï¸ğŸ’¨**

## ğŸ¨ Generate Pretty Charts

```bash
cd scripts/visualization
python create_presentation_charts.py

# Charts will appear in ../../charts/ folder
```

## ğŸ”§ Troubleshooting (Common Issues)

### "Java Error" with PySpark
If you see Java version errors:

1. **Install Java 17+**:
   - Windows: Download from [Oracle](https://www.oracle.com/java/technologies/downloads/)
   - Mac: `brew install openjdk@17`
   - Linux: `sudo apt install openjdk-17-jdk`

2. **Set JAVA_HOME**:
   ```bash
   # Windows
   set JAVA_HOME=C:\Program Files\Java\jdk-17

   # Mac/Linux
   export JAVA_HOME=/usr/lib/jvm/java-17-openjdk
   ```

### "Out of Memory" Errors
- Close other programs
- Start with smaller datasets (1M, 5M) first
- Make sure you have at least 8GB RAM for 50M row tests

### "File Not Found" Errors
The tests automatically generate datasets, but if you get file errors:

```bash
cd scripts/data_generation
python generate_datasets_1m_5m_10m.py
```

## ğŸ¤“ What Operations Do We Test?

Each benchmark runs these common data processing tasks:

1. **Loading** - Reading a CSV file into memory
2. **Cleaning** - Handling missing/null values
3. **Aggregating** - Grouping data and calculating averages
4. **Sorting** - Ordering data by values
5. **Filtering** - Selecting rows that match conditions
6. **Correlations** - Finding relationships between columns

These are the bread-and-butter operations you'll do with any real dataset!

## ğŸ¯ Key Takeaways for Beginners

1. **Polars is usually your best choice** - It's fast, memory-efficient, and has a clean API
2. **Pandas is fine for small data** - But switches to Polars when you hit performance issues
3. **Size matters** - What works for 100K rows might be unusably slow for 10M rows
4. **Memory usage matters** - Some libraries use 3x more RAM than others
5. **Test with your own data** - These results are for our specific test data; your mileage may vary

## ğŸ¤ Need Help?

- **New to data processing?** Start with the 1M row benchmark to see the basics
- **Working with big data?** Focus on the 50M row results
- **Questions?** Open an issue in this repository

## ğŸ“š Want to Learn More?

- [Polars User Guide](https://pola-rs.github.io/polars-book/)
- [Pandas Documentation](https://pandas.pydata.org/docs/)
- [PyArrow Documentation](https://arrow.apache.org/docs/python/)

---

**Happy data processing! ğŸ‰**

*Remember: The best library is the one that solves your specific problem efficiently. But if you're unsure, Polars is a pretty safe bet these days!*